# -*- coding: utf-8 -*-
"""final projact - transformer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WKrIgaQ5IC14Vy7bRlbay8Qpb4sAjMhp
"""

import os
os.makedirs("checkpoints", exist_ok=True)

pip install Gradio

"""××—×‘×¨ ××ª ×”×’×™×˜×”××‘ ×•××•×¨×™×“ ××ª ×”×¡×¤×¨×™×•×ª ×”× ×—×•×¦×•×ª"""

!wget https://raw.githubusercontent.com/yoavjaz/depression-detection-project/main/requirements.txt
!pip install -r requirements.txt

"""××™×™×‘× ××ª ×”×“××˜×”×¡×˜×™× ××§××’×œ"""

import gdown
import zipfile
import os

# ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×œ× ×ª×•× ×™× (×× ×œ× ×§×™×™××ª)
os.makedirs("/content/datasets", exist_ok=True)

# ××–×”×” ×”×§×•×‘×¥ ××ª×•×š ×”×“×¨×™×™×‘
file_id = "11bjXBQT4j54wGRCCPTjH0ZbNk0U7i6GE"
output = "/content/datasets/datasets.zip"

# ×”×•×¨×“×ª ×”×§×•×‘×¥ ××”-Drive
gdown.download(f"https://drive.google.com/uc?id={file_id}", output, quiet=False)

# ×—×™×œ×•×¥ ×”×§×‘×¦×™× ××ª×•×š ×”-ZIP
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall("/content/datasets")

# ×‘×“×™×§×” ××™×œ×• ×§×‘×¦×™× × ×¤×¨×§×•
print("×ª×•×›×Ÿ ×”×ª×™×§×™×™×” ×œ××—×¨ ×”×—×™×œ×•×¥:")
!ls /content/datasets

import zipfile
import os

zip_files = [
    "/content/datasets/sentiment_tweets3.csv.zip",
    "/content/datasets/Suicide_Detection.csv.zip",
    "/content/datasets/depression_dataset_reddit_cleaned.csv.zip"
]

for file in zip_files:
    with zipfile.ZipFile(file, 'r') as zip_ref:
        zip_ref.extractall("/content/datasets")

import pandas as pd
df3 = pd.read_csv("/content/datasets/Combined Data.csv")
df4 = pd.read_csv("/content/datasets/depression_dataset_reddit_cleaned.csv")
df1 = pd.read_csv("/content/datasets/sentiment_tweets3.csv")
df2 = pd.read_csv("/content/datasets/Suicide_Detection.csv")

"""×‘×•×“×§ ××ª ×”×“××˜×” ×¢× ×©×œ×•×© ×“×•×’×××•×ª ×¨××©×•× ×•×ª ××›×œ ××—×ª"""

df1.head(3)

df2.head(3)

df3.head(3)

df4.head(3)

"""×‘×•×“×§ ×›××” ×ª××™× ×¨×™×§×™× ×™×© ×œ×™ ×‘×›×œ ×“××˜×”"""

empty_cells_df1 = df1.isnull().sum()
empty_cells_df2 = df2.isnull().sum()
empty_cells_df3 = df3.isnull().sum()
empty_cells_df4 = df4.isnull().sum()

print("Empty cells in df1:\n", empty_cells_df1)
print("\nEmpty cells in df2:\n", empty_cells_df2)
print("\nEmpty cells in df3:\n", empty_cells_df3)
print("\nEmpty cells in df4:\n", empty_cells_df4)

"""×× ×§×” ××ª ×”×ª××™× ×”×¨×™×§×™× ×©××¦××ª×™"""

df3 = df3.dropna()
print(df3.isnull().sum())

"""×‘×•×“×§ ×›××” ×“×•×’×××•×ª ×©×œ ×˜×§×˜×¡×™× ××ª×—×ª ×œ10 ××™×œ×™× ×™×© ×œ×™ ×‘×›×œ ×“××˜×”, ×× ×™ ×œ× ×¨×•×¦×” ×“×•×’×××•×ª ×›××œ×” ×›×™ ××™ ××¤×©×¨ ×œ×œ××•×“ ××”×Ÿ ×”×¨×‘×”"""

df1['word_count'] = df1['message to examine'].apply(lambda x: len(str(x).split()))
df2['word_count'] = df2['text'].apply(lambda x: len(str(x).split()))
df3['word_count'] = df3['statement'].apply(lambda x: len(str(x).split()))
df4['word_count'] = df4['clean_text'].apply(lambda x: len(str(x).split()))

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

df1 = df1[df1['word_count'] >= 10]
df2 = df2[df2['word_count'] >= 10]
df3 = df3[df3['word_count'] >= 10]
df4 = df4[df4['word_count'] >= 10]

"""×× ×§×” ××ª ×”× ×ª×•× ×™× ×¢× ×¤×—×•×ª ×10 ××™×œ×™× ×•××•×•×“× ×©×”× ×ª×•× ×™× × ×•×§×•"""

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

"""×”×—×œ×¤×ª×™ ××ª ×”×©× ×©×œ ×”×¢××•×“×” ×©×œ ×›×œ ×“××˜×” ×œ×˜×§×¡×˜ ×›×“×™ ×©×™×”×™×• ×ª×•×××•×ª"""

df1.rename(columns={'message to examine': 'text'}, inplace=True)
df3.rename(columns={'statement': 'text'}, inplace=True)
df4.rename(columns={'clean_text': 'text'}, inplace=True)

"""××•×ª×• ×“×‘×¨ ×œ×¢××•×“×ª ×”×œ×™×™×‘×œ"""

df1.rename(columns={'label (depression result)': 'label'}, inplace=True)
df2.rename(columns={'class': 'label'}, inplace=True)
df3.rename(columns={'status': 'label'}, inplace=True)
df4.rename(columns={'is_depression': 'label'}, inplace=True)

"""××©× ×” ××ª ×”× ×ª×•× ×™× ×‘×¢××•×“×ª ×”×œ×™×™×‘×œ ××©× ×œ××¡×¤×¨, 0 ××• 1"""

df2['label'] = df2['label'].map({"non-suicide": 0, "suicide": 1})
df2.head(1)

df3['label'] = df3['label'].map({"Normal": 0, "suicidal": 1, "Anxiety": 1, "Depression":1})
df3.head(1)

"""×‘×•×“×§ ×›××” ×“×•×’×××•×ª ×‘×“×™×›××•×Ÿ ×•×œ× ×‘×“×™×›××•×Ÿ ×™×© ×œ×™"""

label_1_count = df1[df1['label'] == 1].shape[0]
label_2_count = df2[df2['label'] == 1].shape[0]
label_3_count = df3[df3['label'] == 1].shape[0]
label_4_count = df4[df4['label'] == 1].shape[0]
print("the number of depressed text in all of the dataset is:")
print(sum([label_1_count, label_2_count, label_3_count, label_4_count]))


label_1_count_n = df1[df1['label'] == 0].shape[0]
label_2_count_n = df2[df2['label'] == 0].shape[0]
label_3_count_n = df3[df3['label'] == 0].shape[0]
label_4_count_n = df4[df4['label'] == 0].shape[0]
print("the number of not depressed text in all of the dataset is:")
print(sum([label_1_count_n, label_2_count_n, label_3_count_n, label_4_count_n]))

label_2_count = df2[df2['label'] == 1].shape[0]
print(label_2_count)
label_2_count_n = df2[df2['label'] == 0].shape[0]
print(label_2_count_n)

#with zipfile.ZipFile('/content/datasets/reddit_depression_dataset.csv.zip', 'r') as zip_ref:
 #   zip_ref.extractall('/content/datasets')
   # file_path = '/content/datasets/reddit_depression_dataset.csv.zip'

#df5 = pd.read_csv(file_path)

#empty_cells_df5_body = df5['body'].isnull().sum()
#print(f"Number of empty cells in the 'text' column of df3: {empty_cells_df5_body}")

"""×××—×“ ××ª ×›×œ ×”×“××˜×” ×œ×“××˜×” ××—×“"""

df= pd.concat([df1, df2, df3, df4], ignore_index=True)

df= df.sample(frac=1, random_state=42).reset_index(drop=True)

df = df.dropna(subset=["label"])

print(df.isnull().sum())

print(len(df['label']))

print(len(df[df['label'] == 'bipolar']))

"""×‘×•×“×§ ××” ×”×˜×§×¡×˜ ×¢× ×”××•×¨×š ×”××§×¡×™××œ×™"""

max_length = 0
for i in range(len(df)):
    if len(df['text'].iloc[i]) > max_length:
        max_length = len(df['text'].iloc[i])

print(max_length)

"""×‘×•×“×§ ××” ×”×˜×§×¡×˜ ×¢× ×”××•×¨×š ×”××™× ×™××œ×™"""

min_length = 2948719
for i in range(len(df)):
    if len(df['text'].iloc[i]) < min_length:
        min_length = len(df['text'].iloc[i])

print(min_length)

"""×‘×•×“×§ ××” ×”×××•×¦×¢ ×©×œ ×”××•×¨×š ×•×”×—×¦×™×•×Ÿ"""

import numpy as np
num = 0
sum = 0
for i in range(len(df)):
  num = num+1
  sum = sum+len(df['text'].iloc[i])
avg = sum/num
print(avg)
tweet_lengths = np.array(sorted(df['text'].apply(len)))
l = len(tweet_lengths)
median = tweet_lengths[int(l/2)]
print(median)

""" ×× ×§×” ××ª ×”×˜×§×¡×˜ ××¡×™×× ×™ ×¤×™×¡×•×§ ×•××—×œ×™×£ ×”×›×œ ×œ××•×ª×™×•×ª ×§×˜× ×•×ª"""

import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text


df['cleaned_text'] = df['text'].apply(clean_text)
print(df['cleaned_text'][5])

"""×™×•×¦×¨ ×“××˜×”×¡×˜ ×¢× X ×•Y."""

import tensorflow as tf


X = np.array(df['cleaned_text'])
y = np.array(df['label'])
dataset = tf.data.Dataset.from_tensor_slices((X, y))

dataset = dataset.shuffle(buffer_size=10000).batch(32)

"""××—×œ×§ ×œ××™××•×Ÿ ×•××‘×—×Ÿ, ××‘×¦×¢ ×˜×•×§× ×™×–×¨ ××•×ª×× ××¨××© ×©×œ ×‘×¨×˜ ×•×¤××“×™× ×’. ×•×œ××—×¨ ××›×Ÿ ××•×¡×™×£ ×’× ××™×“×¢ ×¢×œ ××™×§×•× ×”××™×œ×” ×•×”×× ×”×™× ×—×œ×§ ××”××©×¤×˜ ××• ××”×¤××“×™× ×’"""

import tensorflow as tf
from transformers import TFBertModel, BertTokenizer
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split


tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


def encode_texts(texts, max_len=300):
    return tokenizer(
        texts.tolist(),
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )


train_encodings = encode_texts(X_train)
val_encodings = encode_texts(X_val)
input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="attention_mask")

print(train_encodings["input_ids"].shape)
print(train_encodings["attention_mask"].shape)

print(train_encodings[:5])  # ×”×“×¤×¡×ª 5 ×“×•×’×××•×ª ×¨××©×•× ×•×ª
print(y_train[:5])  # ×”×“×¤×¡×ª 5 ×ª×•×•×™×•×ª ×¨××©×•× ×•×ª

import numpy as np


print("NaN in y_train:", np.isnan(y_train).sum())

print("Inf in y_train:", np.isinf(y_train).sum())

"""××’×“×™×¨ ××ª ×”××•×“×œ ×¢× ×©×›×‘×ª ×˜×¨× ×¡×¤×•×¨××¨ ×œ××—×¨ ××›×Ÿ ×©×›×‘×ª LSTM ×•×¢×•×“ ×©×›×‘×•×ª dense"""

bert_model = TFBertModel.from_pretrained("bert-base-uncased")


bert_output = tf.keras.layers.Lambda(
    lambda x: bert_model(input_ids=x[0], attention_mask=x[1]).last_hidden_state,
    output_shape=(300, 768)
)([input_ids, attention_mask])


lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(bert_output)









dense1 = tf.keras.layers.Dense(128, activation="relu")(lstm_output)
dropout = tf.keras.layers.Dropout(0.3)(dense1)
dense2 = tf.keras.layers.Dense(64, activation="relu")(dropout)
output = tf.keras.layers.Dense(1, activation="sigmoid")(dense2)


model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)


model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

checkpoint_dir = "checkpoints/best_model_bert.weights.h5"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_dir,
    save_best_only=True,
    monitor='val_accuracy',
    mode='max',
    save_weights_only=True
)

early_stop = EarlyStopping(monitor="val_accuracy", patience=2, mode="max", verbose=1, restore_best_weights=True)

"""××‘×¦×¢ ××ª ×”××™××•×Ÿ ×¢×¦××•"""

history = model.fit(
    {"input_ids": train_encodings["input_ids"], "attention_mask": train_encodings["attention_mask"]},
    y_train,
    validation_data=({"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"]}, y_val),
    epochs=10,
    batch_size=32,
    callbacks=[early_stop, checkpoint_callback]
)

"""××©×—×–×¨ ××ª ××¨×›×™×˜×§×˜×•×¨×ª ×”××•×“×œ ×•××™×™×‘× ××ª ×”××©×§×œ×™× ×©× ×©××¨×•"""

from transformers import TFBertModel
import tensorflow as tf


bert_model = TFBertModel.from_pretrained("bert-base-uncased")


input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="attention_mask")

bert_output = tf.keras.layers.Lambda(
    lambda x: bert_model(input_ids=x[0], attention_mask=x[1]).last_hidden_state,
    output_shape=(300, 768)
)([input_ids, attention_mask])

lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(bert_output)
dense1 = tf.keras.layers.Dense(128, activation="relu")(lstm_output)
dropout = tf.keras.layers.Dropout(0.3)(dense1)
dense2 = tf.keras.layers.Dense(64, activation="relu")(dropout)
output = tf.keras.layers.Dense(1, activation="sigmoid")(dense2)

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)

from tensorflow.keras.models import load_model

model.load_weights("checkpoints/best_model_bert.weights.h5")

"""×‘×•× ×” ×’×¨×£ ×”××œ××“ ×¢×œ ×”×“×™×•×§ ×•×”loss"""

import matplotlib.pyplot as plt


plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()


plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""×‘×•× ×” ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns


y_pred = model.predict({"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"]})
y_pred = (y_pred > 0.5).astype(int)

cm = confusion_matrix(y_val, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""×‘×•× ×” ×¤×•× ×§×¦×™×™×ª ×—×™×–×•×™"""

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer
import re



def predict(text):
    import tensorflow as tf



    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


    cleaned_text = clean_text(text)
    encodings = tokenizer(
        [cleaned_text],
        max_length=300,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )


    prediction = model.predict({"input_ids": encodings["input_ids"], "attention_mask": encodings["attention_mask"]})


    return "most likely not depreesed" if prediction[0][0] < 0.5 else "might have depression"

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text

"""×× ×¡×” ××ª ×”××•×“×œ ×¢×œ ×˜×§×˜×¡×™× ×©×œ×™"""

while True:
  user_input = input("How do you feel in these past few days? ")
  if user_input == "exit":
    break
  prediction = predict(user_input)
  print(f"Prediction: {prediction}")

"""×‘×•× ×” ×××©×§ ××©×ª××© × ×•×—"""

import gradio as gr
from transformers import BertTokenizer
import tensorflow as tf
import numpy as np

# ×˜×•×¢×Ÿ ××ª ×”×˜×•×§× ×™×–×¨
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")



# ×¤×•× ×§×¦×™×” ×œ× ×™×§×•×™ ×•×”××¨×ª ×”×˜×§×¡×˜ ×œ×§×œ×˜ ×œ××•×“×œ
def predict(text):
    encoded = tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=300,
        return_tensors="tf"
    )
    prediction = model.predict({
        "input_ids": encoded["input_ids"],
        "attention_mask": encoded["attention_mask"]
    })[0][0]

    if prediction > 0.5:
        return ("âš ï¸ We detected signs of emotional distress.\n"
                "Please consider talking to someone. You can contact:\n"
                "ğŸ“ Eran - 1201\n"
                "ğŸŒ https://www.eran.org.il/")
    else:
        return "âœ… Everything seems okay. Keep going and take care of yourself!"

# ×”×’×“×¨×ª ×××©×§ Gradio
iface = gr.Interface(
    fn=predict,
    inputs=gr.Textbox(lines=4, placeholder="How was your day today?"),
    outputs="text",
    title="ğŸ§  Depression Detection (BERT model)",
    description="Answer the question and we'll check if there are signs of distress"
)

iface.launch()