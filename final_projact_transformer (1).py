# -*- coding: utf-8 -*-
"""final projact - transformer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WKrIgaQ5IC14Vy7bRlbay8Qpb4sAjMhp
"""

import os
os.makedirs("checkpoints", exist_ok=True)

pip install Gradio

"""מחבר את הגיטהאב ומוריד את הספריות הנחוצות"""

!wget https://raw.githubusercontent.com/yoavjaz/depression-detection-project/main/requirements.txt
!pip install -r requirements.txt

"""מייבא את הדאטהסטים מקאגל"""

import gdown
import zipfile
import os

# יצירת תיקייה לנתונים (אם לא קיימת)
os.makedirs("/content/datasets", exist_ok=True)

# מזהה הקובץ מתוך הדרייב
file_id = "11bjXBQT4j54wGRCCPTjH0ZbNk0U7i6GE"
output = "/content/datasets/datasets.zip"

# הורדת הקובץ מה-Drive
gdown.download(f"https://drive.google.com/uc?id={file_id}", output, quiet=False)

# חילוץ הקבצים מתוך ה-ZIP
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall("/content/datasets")

# בדיקה אילו קבצים נפרקו
print("תוכן התיקייה לאחר החילוץ:")
!ls /content/datasets

import zipfile
import os

zip_files = [
    "/content/datasets/sentiment_tweets3.csv.zip",
    "/content/datasets/Suicide_Detection.csv.zip",
    "/content/datasets/depression_dataset_reddit_cleaned.csv.zip"
]

for file in zip_files:
    with zipfile.ZipFile(file, 'r') as zip_ref:
        zip_ref.extractall("/content/datasets")

import pandas as pd
df3 = pd.read_csv("/content/datasets/Combined Data.csv")
df4 = pd.read_csv("/content/datasets/depression_dataset_reddit_cleaned.csv")
df1 = pd.read_csv("/content/datasets/sentiment_tweets3.csv")
df2 = pd.read_csv("/content/datasets/Suicide_Detection.csv")

"""בודק את הדאטה עם שלוש דוגמאות ראשונות מכל אחת"""

df1.head(3)

df2.head(3)

df3.head(3)

df4.head(3)

"""בודק כמה תאים ריקים יש לי בכל דאטה"""

empty_cells_df1 = df1.isnull().sum()
empty_cells_df2 = df2.isnull().sum()
empty_cells_df3 = df3.isnull().sum()
empty_cells_df4 = df4.isnull().sum()

print("Empty cells in df1:\n", empty_cells_df1)
print("\nEmpty cells in df2:\n", empty_cells_df2)
print("\nEmpty cells in df3:\n", empty_cells_df3)
print("\nEmpty cells in df4:\n", empty_cells_df4)

"""מנקה את התאים הריקים שמצאתי"""

df3 = df3.dropna()
print(df3.isnull().sum())

"""בודק כמה דוגמאות של טקטסים מתחת ל10 מילים יש לי בכל דאטה, אני לא רוצה דוגמאות כאלה כי אי אפשר ללמוד מהן הרבה"""

df1['word_count'] = df1['message to examine'].apply(lambda x: len(str(x).split()))
df2['word_count'] = df2['text'].apply(lambda x: len(str(x).split()))
df3['word_count'] = df3['statement'].apply(lambda x: len(str(x).split()))
df4['word_count'] = df4['clean_text'].apply(lambda x: len(str(x).split()))

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

df1 = df1[df1['word_count'] >= 10]
df2 = df2[df2['word_count'] >= 10]
df3 = df3[df3['word_count'] >= 10]
df4 = df4[df4['word_count'] >= 10]

"""מנקה את הנתונים עם פחות מ10 מילים ומוודא שהנתונים נוקו"""

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

"""החלפתי את השם של העמודה של כל דאטה לטקסט כדי שיהיו תואמות"""

df1.rename(columns={'message to examine': 'text'}, inplace=True)
df3.rename(columns={'statement': 'text'}, inplace=True)
df4.rename(columns={'clean_text': 'text'}, inplace=True)

"""אותו דבר לעמודת הלייבל"""

df1.rename(columns={'label (depression result)': 'label'}, inplace=True)
df2.rename(columns={'class': 'label'}, inplace=True)
df3.rename(columns={'status': 'label'}, inplace=True)
df4.rename(columns={'is_depression': 'label'}, inplace=True)

"""משנה את הנתונים בעמודת הלייבל משם למספר, 0 או 1"""

df2['label'] = df2['label'].map({"non-suicide": 0, "suicide": 1})
df2.head(1)

df3['label'] = df3['label'].map({"Normal": 0, "suicidal": 1, "Anxiety": 1, "Depression":1})
df3.head(1)

"""בודק כמה דוגמאות בדיכאון ולא בדיכאון יש לי"""

label_1_count = df1[df1['label'] == 1].shape[0]
label_2_count = df2[df2['label'] == 1].shape[0]
label_3_count = df3[df3['label'] == 1].shape[0]
label_4_count = df4[df4['label'] == 1].shape[0]
print("the number of depressed text in all of the dataset is:")
print(sum([label_1_count, label_2_count, label_3_count, label_4_count]))


label_1_count_n = df1[df1['label'] == 0].shape[0]
label_2_count_n = df2[df2['label'] == 0].shape[0]
label_3_count_n = df3[df3['label'] == 0].shape[0]
label_4_count_n = df4[df4['label'] == 0].shape[0]
print("the number of not depressed text in all of the dataset is:")
print(sum([label_1_count_n, label_2_count_n, label_3_count_n, label_4_count_n]))

label_2_count = df2[df2['label'] == 1].shape[0]
print(label_2_count)
label_2_count_n = df2[df2['label'] == 0].shape[0]
print(label_2_count_n)

#with zipfile.ZipFile('/content/datasets/reddit_depression_dataset.csv.zip', 'r') as zip_ref:
 #   zip_ref.extractall('/content/datasets')
   # file_path = '/content/datasets/reddit_depression_dataset.csv.zip'

#df5 = pd.read_csv(file_path)

#empty_cells_df5_body = df5['body'].isnull().sum()
#print(f"Number of empty cells in the 'text' column of df3: {empty_cells_df5_body}")

"""מאחד את כל הדאטה לדאטה אחד"""

df= pd.concat([df1, df2, df3, df4], ignore_index=True)

df= df.sample(frac=1, random_state=42).reset_index(drop=True)

df = df.dropna(subset=["label"])

print(df.isnull().sum())

print(len(df['label']))

print(len(df[df['label'] == 'bipolar']))

"""בודק מה הטקסט עם האורך המקסימלי"""

max_length = 0
for i in range(len(df)):
    if len(df['text'].iloc[i]) > max_length:
        max_length = len(df['text'].iloc[i])

print(max_length)

"""בודק מה הטקסט עם האורך המינימלי"""

min_length = 2948719
for i in range(len(df)):
    if len(df['text'].iloc[i]) < min_length:
        min_length = len(df['text'].iloc[i])

print(min_length)

"""בודק מה הממוצע של האורך והחציון"""

import numpy as np
num = 0
sum = 0
for i in range(len(df)):
  num = num+1
  sum = sum+len(df['text'].iloc[i])
avg = sum/num
print(avg)
tweet_lengths = np.array(sorted(df['text'].apply(len)))
l = len(tweet_lengths)
median = tweet_lengths[int(l/2)]
print(median)

""" מנקה את הטקסט מסימני פיסוק ומחליף הכל לאותיות קטנות"""

import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text


df['cleaned_text'] = df['text'].apply(clean_text)
print(df['cleaned_text'][5])

"""יוצר דאטהסט עם X וY."""

import tensorflow as tf


X = np.array(df['cleaned_text'])
y = np.array(df['label'])
dataset = tf.data.Dataset.from_tensor_slices((X, y))

dataset = dataset.shuffle(buffer_size=10000).batch(32)

"""מחלק לאימון ומבחן, מבצע טוקניזר מותאם מראש של ברט ופאדינג. ולאחר מכן מוסיף גם מידע על מיקום המילה והאם היא חלק מהמשפט או מהפאדינג"""

import tensorflow as tf
from transformers import TFBertModel, BertTokenizer
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split


tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


def encode_texts(texts, max_len=300):
    return tokenizer(
        texts.tolist(),
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )


train_encodings = encode_texts(X_train)
val_encodings = encode_texts(X_val)
input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="attention_mask")

print(train_encodings["input_ids"].shape)
print(train_encodings["attention_mask"].shape)

print(train_encodings[:5])  # הדפסת 5 דוגמאות ראשונות
print(y_train[:5])  # הדפסת 5 תוויות ראשונות

import numpy as np


print("NaN in y_train:", np.isnan(y_train).sum())

print("Inf in y_train:", np.isinf(y_train).sum())

"""מגדיר את המודל עם שכבת טרנספורמר לאחר מכן שכבת LSTM ועוד שכבות dense"""

bert_model = TFBertModel.from_pretrained("bert-base-uncased")


bert_output = tf.keras.layers.Lambda(
    lambda x: bert_model(input_ids=x[0], attention_mask=x[1]).last_hidden_state,
    output_shape=(300, 768)
)([input_ids, attention_mask])


lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(bert_output)









dense1 = tf.keras.layers.Dense(128, activation="relu")(lstm_output)
dropout = tf.keras.layers.Dropout(0.3)(dense1)
dense2 = tf.keras.layers.Dense(64, activation="relu")(dropout)
output = tf.keras.layers.Dense(1, activation="sigmoid")(dense2)


model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)


model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

checkpoint_dir = "checkpoints/best_model_bert.weights.h5"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_dir,
    save_best_only=True,
    monitor='val_accuracy',
    mode='max',
    save_weights_only=True
)

early_stop = EarlyStopping(monitor="val_accuracy", patience=2, mode="max", verbose=1, restore_best_weights=True)

"""מבצע את האימון עצמו"""

history = model.fit(
    {"input_ids": train_encodings["input_ids"], "attention_mask": train_encodings["attention_mask"]},
    y_train,
    validation_data=({"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"]}, y_val),
    epochs=10,
    batch_size=32,
    callbacks=[early_stop, checkpoint_callback]
)

"""משחזר את ארכיטקטורת המודל ומייבא את המשקלים שנשמרו"""

from transformers import TFBertModel
import tensorflow as tf


bert_model = TFBertModel.from_pretrained("bert-base-uncased")


input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="attention_mask")

bert_output = tf.keras.layers.Lambda(
    lambda x: bert_model(input_ids=x[0], attention_mask=x[1]).last_hidden_state,
    output_shape=(300, 768)
)([input_ids, attention_mask])

lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(bert_output)
dense1 = tf.keras.layers.Dense(128, activation="relu")(lstm_output)
dropout = tf.keras.layers.Dropout(0.3)(dense1)
dense2 = tf.keras.layers.Dense(64, activation="relu")(dropout)
output = tf.keras.layers.Dense(1, activation="sigmoid")(dense2)

model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)

from tensorflow.keras.models import load_model

model.load_weights("checkpoints/best_model_bert.weights.h5")

"""בונה גרף המלמד על הדיוק והloss"""

import matplotlib.pyplot as plt


plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()


plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""בונה מטריצת בלבול"""

import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns


y_pred = model.predict({"input_ids": val_encodings["input_ids"], "attention_mask": val_encodings["attention_mask"]})
y_pred = (y_pred > 0.5).astype(int)

cm = confusion_matrix(y_val, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

"""בונה פונקציית חיזוי"""

import numpy as np
import tensorflow as tf
from transformers import BertTokenizer
import re



def predict(text):
    import tensorflow as tf



    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


    cleaned_text = clean_text(text)
    encodings = tokenizer(
        [cleaned_text],
        max_length=300,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )


    prediction = model.predict({"input_ids": encodings["input_ids"], "attention_mask": encodings["attention_mask"]})


    return "most likely not depreesed" if prediction[0][0] < 0.5 else "might have depression"

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text

"""מנסה את המודל על טקטסים שלי"""

while True:
  user_input = input("How do you feel in these past few days? ")
  if user_input == "exit":
    break
  prediction = predict(user_input)
  print(f"Prediction: {prediction}")

"""בונה ממשק משתמש נוח"""

import gradio as gr
from transformers import BertTokenizer
import tensorflow as tf
import numpy as np

# טוען את הטוקניזר
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")



# פונקציה לניקוי והמרת הטקסט לקלט למודל
def predict(text):
    encoded = tokenizer(
        text,
        padding='max_length',
        truncation=True,
        max_length=300,
        return_tensors="tf"
    )
    prediction = model.predict({
        "input_ids": encoded["input_ids"],
        "attention_mask": encoded["attention_mask"]
    })[0][0]

    if prediction > 0.5:
        return ("⚠️ We detected signs of emotional distress.\n"
                "Please consider talking to someone. You can contact:\n"
                "📞 Eran - 1201\n"
                "🌐 https://www.eran.org.il/")
    else:
        return "✅ Everything seems okay. Keep going and take care of yourself!"

# הגדרת ממשק Gradio
iface = gr.Interface(
    fn=predict,
    inputs=gr.Textbox(lines=4, placeholder="How was your day today?"),
    outputs="text",
    title="🧠 Depression Detection (BERT model)",
    description="Answer the question and we'll check if there are signs of distress"
)

iface.launch()