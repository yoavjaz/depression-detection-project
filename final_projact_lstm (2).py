# -*- coding: utf-8 -*-
"""final projact - LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F78e00WZjrTb5ou9bNqTUfQ64F9s1oxK
"""

import os
os.makedirs("checkpoints", exist_ok=True)

pip install Gradio

"""××—×‘×¨ ××ª ×”×’×™×˜×”××‘ ×•××•×¨×™×“ ××ª ×”×¡×¤×¨×™×•×ª ×”× ×—×•×¦×•×ª"""

!wget https://raw.githubusercontent.com/yoavjaz/depression-detection-project/main/requirements.txt
!pip install -r requirements.txt

"""××™×™×‘× ××ª ×”×“××˜×”×¡×˜×™× ××§××’×œ"""

import gdown
import zipfile
import os

# ×™×¦×™×¨×ª ×ª×™×§×™×™×” ×œ× ×ª×•× ×™× (×× ×œ× ×§×™×™××ª)
os.makedirs("/content/datasets", exist_ok=True)

# ××–×”×” ×”×§×•×‘×¥ ××ª×•×š ×”×“×¨×™×™×‘
file_id = "11bjXBQT4j54wGRCCPTjH0ZbNk0U7i6GE"
output = "/content/datasets/datasets.zip"

# ×”×•×¨×“×ª ×”×§×•×‘×¥ ××”-Drive
gdown.download(f"https://drive.google.com/uc?id={file_id}", output, quiet=False)

# ×—×™×œ×•×¥ ×”×§×‘×¦×™× ××ª×•×š ×”-ZIP
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall("/content/datasets")

# ×‘×“×™×§×” ××™×œ×• ×§×‘×¦×™× × ×¤×¨×§×•
print("×ª×•×›×Ÿ ×”×ª×™×§×™×™×” ×œ××—×¨ ×”×—×™×œ×•×¥:")
!ls /content/datasets

import zipfile
import os

zip_files = [
    "/content/datasets/sentiment_tweets3.csv.zip",
    "/content/datasets/Suicide_Detection.csv.zip",
    "/content/datasets/depression_dataset_reddit_cleaned.csv.zip"
]

for file in zip_files:
    with zipfile.ZipFile(file, 'r') as zip_ref:
        zip_ref.extractall("/content/datasets")

import pandas as pd
df3 = pd.read_csv("/content/datasets/Combined Data.csv")
df4 = pd.read_csv("/content/datasets/depression_dataset_reddit_cleaned.csv")
df1 = pd.read_csv("/content/datasets/sentiment_tweets3.csv")
df2 = pd.read_csv("/content/datasets/Suicide_Detection.csv")

print("âœ… Combined Data:", df3.shape)
print("âœ… Reddit Depression:", df4.shape)
print("âœ… Tweets3:", df1.shape)
print("âœ… Suicide Detection:", df2.shape)

"""×‘×•×“×§ ××ª ×”×“××˜×” ×¢× ×©×œ×•×© ×“×•×’×××•×ª ×¨××©×•× ×•×ª ××›×œ ××—×ª"""

df1.head(3)

df2.head(3)

df3.head(3)

df4.head(3)

"""×‘×•×“×§ ×›××” ×ª××™× ×¨×™×§×™× ×™×© ×œ×™ ×‘×›×œ ×“××˜×”"""

empty_cells_df1 = df1.isnull().sum()
empty_cells_df2 = df2.isnull().sum()
empty_cells_df3 = df3.isnull().sum()
empty_cells_df4 = df4.isnull().sum()

print("Empty cells in df1:\n", empty_cells_df1)
print("\nEmpty cells in df2:\n", empty_cells_df2)
print("\nEmpty cells in df3:\n", empty_cells_df3)
print("\nEmpty cells in df4:\n", empty_cells_df4)

"""×× ×§×” ××ª ×”×ª××™× ×”×¨×™×§×™× ×©××¦××ª×™"""

df3 = df3.dropna()
print(df3.isnull().sum())

"""×‘×•×“×§ ×›××” ×“×•×’×××•×ª ×©×œ ×˜×§×˜×¡×™× ××ª×—×ª ×œ10 ××™×œ×™× ×™×© ×œ×™ ×‘×›×œ ×“××˜×”, ×× ×™ ×œ× ×¨×•×¦×” ×“×•×’×××•×ª ×›××œ×” ×›×™ ××™ ××¤×©×¨ ×œ×œ××•×“ ××”×Ÿ ×”×¨×‘×”"""

df1['word_count'] = df1['message to examine'].apply(lambda x: len(str(x).split()))
df2['word_count'] = df2['text'].apply(lambda x: len(str(x).split()))
df3['word_count'] = df3['statement'].apply(lambda x: len(str(x).split()))
df4['word_count'] = df4['clean_text'].apply(lambda x: len(str(x).split()))

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

df1 = df1[df1['word_count'] >= 10]
df2 = df2[df2['word_count'] >= 10]
df3 = df3[df3['word_count'] >= 10]
df4 = df4[df4['word_count'] >= 10]

"""×× ×§×” ××ª ×”× ×ª×•× ×™× ×¢× ×¤×—×•×ª ×10 ××™×œ×™× ×•××•×•×“× ×©×”× ×ª×•× ×™× × ×•×§×•"""

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

"""×”×—×œ×¤×ª×™ ××ª ×”×©× ×©×œ ×”×¢××•×“×” ×©×œ ×›×œ ×“××˜×” ×œ×˜×§×¡×˜ ×›×“×™ ×©×™×”×™×• ×ª×•×××•×ª"""

df1.rename(columns={'message to examine': 'text'}, inplace=True)
df3.rename(columns={'statement': 'text'}, inplace=True)
df4.rename(columns={'clean_text': 'text'}, inplace=True)

"""××•×ª×• ×“×‘×¨ ×œ×¢××•×“×ª ×”×œ×™×™×‘×œ"""

df1.rename(columns={'label (depression result)': 'label'}, inplace=True)
df2.rename(columns={'class': 'label'}, inplace=True)
df3.rename(columns={'status': 'label'}, inplace=True)
df4.rename(columns={'is_depression': 'label'}, inplace=True)

"""××©× ×” ××ª ×”× ×ª×•× ×™× ×‘×¢××•×“×ª ×”×œ×™×™×‘×œ ××©× ×œ××¡×¤×¨, 0 ××• 1"""

df2['label'] = df2['label'].map({"non-suicide": 0, "suicide": 1})
df2.head(1)

df3['label'] = df3['label'].map({"Normal": 0, "suicidal": 1, "Anxiety": 1, "Depression":1})
df3.head(1)

"""×‘×•×“×§ ×›××” ×“×•×’×××•×ª ×‘×“×™×›××•×Ÿ ×•×œ× ×‘×“×™×›××•×Ÿ ×™×© ×œ×™"""

label_1_count = df1[df1['label'] == 1].shape[0]
label_2_count = df2[df2['label'] == 1].shape[0]
label_3_count = df3[df3['label'] == 1].shape[0]
label_4_count = df4[df4['label'] == 1].shape[0]
print("the number of depressed text in all of the dataset is:")
print(sum([label_1_count, label_2_count, label_3_count, label_4_count]))


label_1_count_n = df1[df1['label'] == 0].shape[0]
label_2_count_n = df2[df2['label'] == 0].shape[0]
label_3_count_n = df3[df3['label'] == 0].shape[0]
label_4_count_n = df4[df4['label'] == 0].shape[0]
print("the number of not depressed text in all of the dataset is:")
print(sum([label_1_count_n, label_2_count_n, label_3_count_n, label_4_count_n]))

"""×××—×“ ××ª ×›×œ ×”×“××˜×” ×œ×“××˜×” ××—×“"""

df= pd.concat([df1, df2, df3, df4], ignore_index=True)

df= df.sample(frac=1, random_state=42).reset_index(drop=True)

print(len(df['label']))

"""×‘×•×“×§ ××” ×”×˜×§×¡×˜ ×¢× ×”××•×¨×š ×”××§×¡×™××œ×™"""

max_length = 0
for i in range(len(df)):
    if len(df['text'].iloc[i]) > max_length:
        max_length = len(df['text'].iloc[i])

print(max_length)

"""×‘×•×“×§ ××” ×”×˜×§×¡×˜ ×¢× ×”××•×¨×š ×”××™× ×™××œ×™"""

min_length = 2948719
for i in range(len(df)):
    if len(df['text'].iloc[i]) < min_length:
        min_length = len(df['text'].iloc[i])

print(min_length)

"""×‘×•×“×§ ××” ×”×××•×¦×¢ ×©×œ ×”××•×¨×š ×•×”×—×¦×™×•×Ÿ"""

import numpy as np
num = 0
sum = 0
for i in range(len(df)):
  num = num+1
  sum = sum+len(df['text'].iloc[i])
avg = sum/num
print(avg)
tweet_lengths = np.array(sorted(df['text'].apply(len)))
l = len(tweet_lengths)
median = tweet_lengths[int(l/2)]
print(median)

"""×× ×§×” ××ª ×›×œ ×”×“××˜×” ××•×¨×™×“ ×¡×™×× ×™ ×¤×™×¡×•×§ ×•××—×œ×™×£ ××•×ª×™×•×ª ×’×“×•×œ×•×ª ×‘×§×˜× ×•×ª, ×œ××—×¨ ××›×Ÿ ××‘×¦×¢ ×˜×•×§× ×™×–×¦×™×” ×©×œ 20000 ××™×œ×™× ×¢×œ ×”×˜×§×¡×˜."""

import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text


df['cleaned_text'] = df['text'].apply(clean_text)
print(df['cleaned_text'][5])

"""×™×•×¦×¨ ×“××˜×”×¡×˜ ×¢× X ×•Y."""

import pandas as pd


df = df.dropna(subset=['label'])


X = df['cleaned_text']
y = df['label']


print(f"××¡×¤×¨ ×“×•×’×××•×ª ×‘-X: {len(X)}")
print(f"××¡×¤×¨ ×“×•×’×××•×ª ×‘-Y: {len(y)}")
assert len(X) == len(y), "âš ï¸ ×©×’×™××”! ××¡×¤×¨ ×”×“×•×’×××•×ª ×‘-X ×•×‘-Y ×œ× ×ª×•××."

"""××‘×¦×¢  ×˜×•×§× ×–×™× ×’ ×•×¤××“×™× ×’ ×¢×œ ×›×œ ×”×˜×§×¡×˜ ×‘××•×¨×š ×©×œ ×”×—×¦×™×•×Ÿ, 331 ××™×œ×™×. ×•×˜×•×§× ×™×–×¨ ××•×’×“×¨ ×¢×œ 20000 ××™×œ×™×.

> Add blockquote


"""

import tensorflow as tf
import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
padded_sequences = pad_sequences(sequences, int((median)), padding='post')
print(padded_sequences[89])
import pickle

# ×©××™×¨×ª ×”×˜×•×§× ×™×–×¨
with open('checkpoints/tokenizer_lstm.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

X = padded_sequences
y = df['label']

"""××‘×¦×¢ ××ª ×”××™××•×Ÿ ×¢× ×©×›×‘×ª ×××‘×“×™× ×’, ×©×›×‘×ª LSTM ×“×• ×›×™×•×•× ×™×ª ×•×¢× ××•×¤×˜×™××™×–×¦×™×™×ª ××“× ×•×¤×•× ×§×¦×™×™×ª ×©×’×™××” ×§×¨×•×¡ ×× ×˜×¨×•×¤×™"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=20000, output_dim=128, input_length=int((median))),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

checkpoint_dir = "checkpoints/best_model_lstm.h5"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_dir,
    save_best_only=True,
    monitor='val_accuracy',
    mode='max',
    save_weights_only=False
)
early_stop = EarlyStopping(monitor="val_accuracy", patience=2, mode="max", verbose=1, restore_best_weights=True)


history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), callbacks=[checkpoint_callback, early_stop])

"""×‘×•× ×” ××˜×¨×™×¦×ª ×‘×œ×‘×•×œ"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix


y_pred = model.predict(X_val)
y_pred_classes = (y_pred > 0.5).astype(int)


conf_matrix = confusion_matrix(y_val, y_pred_classes)


plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Depressed', 'Depressed'],
            yticklabels=['Not Depressed', 'Depressed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# prompt: generate me a learning graph of val acc and loss

import matplotlib.pyplot as plt

# Assuming 'history' is the object returned by model.fit
plt.figure(figsize=(10, 5))

# Plot training & validation accuracy values
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()
plt.show()

"""×‘×•× ×” ×¤×•× ×§×¦×™×™×ª ×¡×™×•×•×’ ×©××©×ª××©×ª ×‘××•×“×œ ×©×™×¦×¨× ×• ×•××–×”×” ×× ×˜×§×¡×˜ ×©× ×›× ×¡ ××œ×™×” ×”×•× ×‘×Ÿ ××“× ×‘×“×™×›××•×Ÿ"""

# ×˜×¢×™× ×ª ×”××•×“×œ
from tensorflow.keras.models import load_model

import tensorflow as tf
import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
model = load_model("checkpoints/best_model_lstm.h5")


def predict_sentiment(text):
    """
    ×× ×‘× ××ª ×”×¡× ×˜×™×× ×˜ (×—×™×•×‘×™ ××• ×©×œ×™×œ×™) ×œ××©×¤×˜ × ×ª×•×Ÿ.
    """

    def clean_text(text):
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower()
        return text

    cleaned_text = clean_text(text)
    import pickle

    with open('checkpoints/tokenizer_lstm.pkl', 'rb') as handle:
      tokenizer = pickle.load(handle)



    sequences = tokenizer.texts_to_sequences([cleaned_text])


    max_length = 25
    padded_sequence = pad_sequences(sequences, maxlen=max_length, padding='post')


    prediction = model.predict(padded_sequence)[0][0]


    if prediction >= 0.5:
        return "might be depressed"
    else:
        return "most likely not depressed"

"""×‘×•×“×§ ××ª ×”××•×“×œ ×©×œ×™ ×¢× ×“×•×’×××•×ª ×©×œ ×¢×¦××™"""

sentence_1 = "I feel very bad. I had a nightmare and I cant take it anymore."
sentence_2 = "i am so happy right now"
print("sentence number 1 is a man who",predict_sentiment(sentence_1))
print("sentence number 2 is a man who",predict_sentiment(sentence_2))

import gradio as gr
import tensorflow as tf
import numpy as np
import pickle
import re
from tensorflow.keras.preprocessing.sequence import pad_sequences


model = tf.keras.models.load_model("checkpoints/best_model_lstm.h5")

with open("checkpoints/tokenizer_lstm.pkl", "rb") as handle:
    tokenizer = pickle.load(handle)


MAX_LEN = 35

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text


def predict_lstm(text):
    cleaned = clean_text(text)
    sequence = tokenizer.texts_to_sequences([cleaned])
    padded = pad_sequences(sequence, maxlen=MAX_LEN, padding='post')
    prediction = model.predict(padded)[0][0]

    if prediction > 0.5:
        return ("âš ï¸ We detected signs of emotional distress.\n"
                "Please consider talking to someone. You can contact:\n"
                "ğŸ“ Eran - 1201\n"
                "ğŸŒ https://www.eran.org.il/")
    else:
        return "âœ… Everything seems okay. Keep going and take care of yourself!"


iface = gr.Interface(
    fn=predict_lstm,
    inputs=gr.Textbox(lines=4, placeholder="How was your day today?"),
    outputs="text",
    title="ğŸ§  Depression Detection (LSTM Model)",
    description="Answer the question and we'll check if there are signs of distress"
)

iface.launch()