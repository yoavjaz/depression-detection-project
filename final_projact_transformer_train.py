# -*- coding: utf-8 -*-
"""final projact - transformer - train

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YFT7j45YGAiUKxdkvZ3MWM5B2oTU_N00
"""

df1.head(3)

df2.head(3)

df3.head(3)

df4.head(3)

empty_cells_df1 = df1.isnull().sum()
empty_cells_df2 = df2.isnull().sum()
empty_cells_df3 = df3.isnull().sum()
empty_cells_df4 = df4.isnull().sum()

print("Empty cells in df1:\n", empty_cells_df1)
print("\nEmpty cells in df2:\n", empty_cells_df2)
print("\nEmpty cells in df3:\n", empty_cells_df3)
print("\nEmpty cells in df4:\n", empty_cells_df4)

df3 = df3.dropna()
print(df3.isnull().sum())

df1['word_count'] = df1['message to examine'].apply(lambda x: len(str(x).split()))
df2['word_count'] = df2['text'].apply(lambda x: len(str(x).split()))
df3['word_count'] = df3['statement'].apply(lambda x: len(str(x).split()))
df4['word_count'] = df4['clean_text'].apply(lambda x: len(str(x).split()))

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

df1 = df1[df1['word_count'] >= 10]
df2 = df2[df2['word_count'] >= 10]
df3 = df3[df3['word_count'] >= 10]
df4 = df4[df4['word_count'] >= 10]

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

df1.rename(columns={'message to examine': 'text'}, inplace=True)
df3.rename(columns={'statement': 'text'}, inplace=True)
df4.rename(columns={'clean_text': 'text'}, inplace=True)

df1.rename(columns={'label (depression result)': 'label'}, inplace=True)
df2.rename(columns={'class': 'label'}, inplace=True)
df3.rename(columns={'status': 'label'}, inplace=True)
df4.rename(columns={'is_depression': 'label'}, inplace=True)

df2['label'] = df2['label'].map({"non-suicide": 0, "suicide": 1})
df2.head(1)

df3['label'] = df3['label'].map({"Normal": 0, "suicidal": 1, "Anxiety": 1, "Depression":1})
df3.head(1)

label_1_count = df1[df1['label'] == 1].shape[0]
label_2_count = df2[df2['label'] == 1].shape[0]
label_3_count = df3[df3['label'] == 1].shape[0]
label_4_count = df4[df4['label'] == 1].shape[0]
print("the number of depressed text in all of the dataset is:")
print(sum([label_1_count, label_2_count, label_3_count, label_4_count]))


label_1_count_n = df1[df1['label'] == 0].shape[0]
label_2_count_n = df2[df2['label'] == 0].shape[0]
label_3_count_n = df3[df3['label'] == 0].shape[0]
label_4_count_n = df4[df4['label'] == 0].shape[0]
print("the number of not depressed text in all of the dataset is:")
print(sum([label_1_count_n, label_2_count_n, label_3_count_n, label_4_count_n]))

label_2_count = df2[df2['label'] == 1].shape[0]
print(label_2_count)
label_2_count_n = df2[df2['label'] == 0].shape[0]
print(label_2_count_n)

#with zipfile.ZipFile('/content/datasets/reddit_depression_dataset.csv.zip', 'r') as zip_ref:
 #   zip_ref.extractall('/content/datasets')
   # file_path = '/content/datasets/reddit_depression_dataset.csv.zip'

#df5 = pd.read_csv(file_path)

#empty_cells_df5_body = df5['body'].isnull().sum()
#print(f"Number of empty cells in the 'text' column of df3: {empty_cells_df5_body}")

df= pd.concat([df1, df2, df3, df4], ignore_index=True)

df= df.sample(frac=1, random_state=42).reset_index(drop=True)

df = df.dropna(subset=["label"])

print(df.isnull().sum())

print(len(df['label']))

print(len(df[df['label'] == 'bipolar']))

max_length = 0
for i in range(len(df)):
    if len(df['text'].iloc[i]) > max_length:
        max_length = len(df['text'].iloc[i])

print(max_length)

min_length = 2948719
for i in range(len(df)):
    if len(df['text'].iloc[i]) < min_length:
        min_length = len(df['text'].iloc[i])

print(min_length)

import numpy as np
num = 0
sum = 0
for i in range(len(df)):
  num = num+1
  sum = sum+len(df['text'].iloc[i])
avg = sum/num
print(avg)
tweet_lengths = np.array(sorted(df['text'].apply(len)))
l = len(tweet_lengths)
median = tweet_lengths[int(l/2)]
print(median)

import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text


df['cleaned_text'] = df['text'].apply(clean_text)
print(df['cleaned_text'][5])

import tensorflow as tf


X = np.array(df['cleaned_text'])
y = np.array(df['label'])
dataset = tf.data.Dataset.from_tensor_slices((X, y))

dataset = dataset.shuffle(buffer_size=10000).batch(32)

import tensorflow as tf
from transformers import TFBertModel, BertTokenizer
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split


tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")


X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)


def encode_texts(texts, max_len=300):
    return tokenizer(
        texts.tolist(),
        max_length=max_len,
        padding='max_length',
        truncation=True,
        return_tensors="tf"
    )


train_encodings = encode_texts(X_train)
val_encodings = encode_texts(X_val)
input_ids = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(300,), dtype=tf.int32, name="attention_mask")

print(train_encodings["input_ids"].shape)
print(train_encodings["attention_mask"].shape)

print(train_encodings[:5])  # הדפסת 5 דוגמאות ראשונות
print(y_train[:5])  # הדפסת 5 תוויות ראשונות

import numpy as np


print("NaN in y_train:", np.isnan(y_train).sum())

print("Inf in y_train:", np.isinf(y_train).sum())

bert_model = TFBertModel.from_pretrained("bert-base-uncased")


bert_output = tf.keras.layers.Lambda(
    lambda x: bert_model(input_ids=x[0], attention_mask=x[1]).last_hidden_state,
    output_shape=(300, 768)
)([input_ids, attention_mask])


lstm_output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(bert_output)









dense1 = tf.keras.layers.Dense(128, activation="relu")(lstm_output)
dropout = tf.keras.layers.Dropout(0.3)(dense1)
dense2 = tf.keras.layers.Dense(64, activation="relu")(dropout)
output = tf.keras.layers.Dense(1, activation="sigmoid")(dense2)


model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)


model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

checkpoint_dir = "/content/drive/MyDrive/checkpoints/best_model.h5"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_dir,
    save_best_only=True,
    monitor='val_accuracy',
    mode='max',
    save_weights_only=True,
    verbose=1
)

early_stop = EarlyStopping(monitor="val_accuracy", patience=2, mode="max", verbose=1, restore_best_weights=True)