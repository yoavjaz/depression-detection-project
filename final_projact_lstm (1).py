# -*- coding: utf-8 -*-
"""final projact - LSTM

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F78e00WZjrTb5ou9bNqTUfQ64F9s1oxK
"""

import os
os.makedirs("checkpoints", exist_ok=True)

import gdown
import zipfile
import os

# יצירת תיקייה לנתונים (אם לא קיימת)
os.makedirs("/content/datasets", exist_ok=True)

# מזהה הקובץ מתוך הדרייב
file_id = "11bjXBQT4j54wGRCCPTjH0ZbNk0U7i6GE"
output = "/content/datasets/datasets.zip"

# הורדת הקובץ מה-Drive
gdown.download(f"https://drive.google.com/uc?id={file_id}", output, quiet=False)

# חילוץ הקבצים מתוך ה-ZIP
with zipfile.ZipFile(output, 'r') as zip_ref:
    zip_ref.extractall("/content/datasets")

# בדיקה אילו קבצים נפרקו
print("תוכן התיקייה לאחר החילוץ:")
!ls /content/datasets

import zipfile
import os

zip_files = [
    "/content/datasets/sentiment_tweets3.csv.zip",
    "/content/datasets/Suicide_Detection.csv.zip",
    "/content/datasets/depression_dataset_reddit_cleaned.csv.zip"
]

for file in zip_files:
    with zipfile.ZipFile(file, 'r') as zip_ref:
        zip_ref.extractall("/content/datasets")

df3 = pd.read_csv("/content/datasets/Combined Data.csv")
df4 = pd.read_csv("/content/datasets/depression_dataset_reddit_cleaned.csv")
df1 = pd.read_csv("/content/datasets/sentiment_tweets3.csv")
df2 = pd.read_csv("/content/datasets/Suicide_Detection.csv")

print("✅ Combined Data:", df3.shape)
print("✅ Reddit Depression:", df4.shape)
print("✅ Tweets3:", df1.shape)
print("✅ Suicide Detection:", df2.shape)

"""מייבא את הדאטהסטים מקאגל

בודק את הדאטה עם שלוש דוגמאות ראשונות מכל אחת
"""

df1.head(3)

df2.head(3)

df3.head(3)

df4.head(3)

"""בודק כמה תאים ריקים יש לי בכל דאטה"""

empty_cells_df1 = df1.isnull().sum()
empty_cells_df2 = df2.isnull().sum()
empty_cells_df3 = df3.isnull().sum()
empty_cells_df4 = df4.isnull().sum()

print("Empty cells in df1:\n", empty_cells_df1)
print("\nEmpty cells in df2:\n", empty_cells_df2)
print("\nEmpty cells in df3:\n", empty_cells_df3)
print("\nEmpty cells in df4:\n", empty_cells_df4)

"""מנקה את התאים הריקים שמצאתי"""

df3 = df3.dropna()
print(df3.isnull().sum())

"""בודק כמה דוגמאות של טקטסים מתחת ל10 מילים יש לי בכל דאטה, אני לא רוצה דוגמאות כאלה כי אי אפשר ללמוד מהן הרבה"""

df1['word_count'] = df1['message to examine'].apply(lambda x: len(str(x).split()))
df2['word_count'] = df2['text'].apply(lambda x: len(str(x).split()))
df3['word_count'] = df3['statement'].apply(lambda x: len(str(x).split()))
df4['word_count'] = df4['clean_text'].apply(lambda x: len(str(x).split()))

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

df1 = df1[df1['word_count'] >= 10]
df2 = df2[df2['word_count'] >= 10]
df3 = df3[df3['word_count'] >= 10]
df4 = df4[df4['word_count'] >= 10]

"""מנקה את הנתונים עם פחות מ10 מילים ומוודא שהנתונים נוקו"""

less_than_10_df1 = len(df1[df1['word_count'] < 10])
less_than_10_df2 = len(df2[df2['word_count'] < 10])
less_than_10_df3 = len(df3[df3['word_count'] < 10])
less_than_10_df4 = len(df4[df4['word_count'] < 10])

print(f"Number of texts with less than 10 words in df1: {less_than_10_df1}")
print(f"Number of texts with less than 10 words in df2: {less_than_10_df2}")
print(f"Number of texts with less than 10 words in df3: {less_than_10_df3}")
print(f"Number of texts with less than 10 words in df4: {less_than_10_df4}")

"""החלפתי את השם של העמודה של כל דאטה לטקסט כדי שיהיו תואמות"""

df1.rename(columns={'message to examine': 'text'}, inplace=True)
df3.rename(columns={'statement': 'text'}, inplace=True)
df4.rename(columns={'clean_text': 'text'}, inplace=True)

"""אותו דבר לעמודת הלייבל"""

df1.rename(columns={'label (depression result)': 'label'}, inplace=True)
df2.rename(columns={'class': 'label'}, inplace=True)
df3.rename(columns={'status': 'label'}, inplace=True)
df4.rename(columns={'is_depression': 'label'}, inplace=True)

"""משנה את הנתונים בעמודת הלייבל משם למספר, 0 או 1"""

df2['label'] = df2['label'].map({"non-suicide": 0, "suicide": 1})
df2.head(1)

df3['label'] = df3['label'].map({"Normal": 0, "suicidal": 1, "Anxiety": 1, "Depression":1})
df3.head(1)

"""בודק כמה דוגמאות בדיכאון ולא בדיכאון יש לי"""

label_1_count = df1[df1['label'] == 1].shape[0]
label_2_count = df2[df2['label'] == 1].shape[0]
label_3_count = df3[df3['label'] == 1].shape[0]
label_4_count = df4[df4['label'] == 1].shape[0]
print("the number of depressed text in all of the dataset is:")
print(sum([label_1_count, label_2_count, label_3_count, label_4_count]))


label_1_count_n = df1[df1['label'] == 0].shape[0]
label_2_count_n = df2[df2['label'] == 0].shape[0]
label_3_count_n = df3[df3['label'] == 0].shape[0]
label_4_count_n = df4[df4['label'] == 0].shape[0]
print("the number of not depressed text in all of the dataset is:")
print(sum([label_1_count_n, label_2_count_n, label_3_count_n, label_4_count_n]))

"""מאחד את כל הדאטה לדאטה אחד"""

df= pd.concat([df1, df2, df3, df4], ignore_index=True)

df= df.sample(frac=1, random_state=42).reset_index(drop=True)

print(len(df['label']))

"""בודק מה הטקסט עם האורך המקסימלי"""

max_length = 0
for i in range(len(df)):
    if len(df['text'].iloc[i]) > max_length:
        max_length = len(df['text'].iloc[i])

print(max_length)

"""בודק מה הטקסט עם האורך המינימלי"""

min_length = 2948719
for i in range(len(df)):
    if len(df['text'].iloc[i]) < min_length:
        min_length = len(df['text'].iloc[i])

print(min_length)

"""בודק מה הממוצע של האורך והחציון"""

import numpy as np
num = 0
sum = 0
for i in range(len(df)):
  num = num+1
  sum = sum+len(df['text'].iloc[i])
avg = sum/num
print(avg)
tweet_lengths = np.array(sorted(df['text'].apply(len)))
l = len(tweet_lengths)
median = tweet_lengths[int(l/2)]
print(median)

"""מנקה את כל הדאטה מוריד סימני פיסוק ומחליף אותיות גדולות בקטנות, לאחר מכן מבצע טוקניזציה של 20000 מילים על הטקסט."""

import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = text.lower()
    return text


df['cleaned_text'] = df['text'].apply(clean_text)
print(df['cleaned_text'][5])

"""יוצר דאטהסט עם X וY."""

import pandas as pd


df = df.dropna(subset=['label'])


X = df['cleaned_text']
y = df['label']


print(f"מספר דוגמאות ב-X: {len(X)}")
print(f"מספר דוגמאות ב-Y: {len(y)}")
assert len(X) == len(y), "⚠️ שגיאה! מספר הדוגמאות ב-X וב-Y לא תואם."

"""מבצע  טוקנזינג ופאדינג על כל הטקסט באורך של החציון, 331 מילים. וטוקניזר מוגדר על 20000 מילים.

> Add blockquote


"""

import tensorflow as tf
import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=20000)
tokenizer.fit_on_texts(X)
sequences = tokenizer.texts_to_sequences(X)
padded_sequences = pad_sequences(sequences, int((median)), padding='post')
print(padded_sequences[89])
import pickle

# שמירת הטוקניזר
with open('checkpoints/tokenizer_lstm.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

X = padded_sequences
y = df['label']

"""מבצע את האימון עם שכבת אמבדינג, שכבת LSTM דו כיוונית ועם אופטימיזציית אדם ופונקציית שגיאה קרוס אנטרופי"""

from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=20000, output_dim=128, input_length=int((median))),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

checkpoint_dir = "checkpoints/best_model_lstm.h5"
checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_dir,
    save_best_only=True,
    monitor='val_accuracy',
    mode='max',
    save_weights_only=False
)
early_stop = EarlyStopping(monitor="val_accuracy", patience=2, mode="max", verbose=1, restore_best_weights=True)


history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_val, y_val), callbacks=[checkpoint_callback, early_stop])

"""בונה מטריצת בלבול"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix


y_pred = model.predict(X_val)
y_pred_classes = (y_pred > 0.5).astype(int)


conf_matrix = confusion_matrix(y_val, y_pred_classes)


plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Depressed', 'Depressed'],
            yticklabels=['Not Depressed', 'Depressed'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""בונה פונקציית סיווג שמשתמשת במודל שיצרנו ומזהה אם טקסט שנכנס אליה הוא בן אדם בדיכאון"""

# טעינת המודל
import tensorflow as tf
import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
model = load_model("checkpoints/best_model_lstm.h5")


def predict_sentiment(text):
    """
    מנבא את הסנטימנט (חיובי או שלילי) למשפט נתון.
    """

    def clean_text(text):
        text = re.sub(r'[^\w\s]', '', text)
        text = text.lower()
        return text

    cleaned_text = clean_text(text)
    import pickle

    with open('checkpoints/tokenizer_lstm.pkl', 'rb') as handle:
      tokenizer = pickle.load(handle)



    sequences = tokenizer.texts_to_sequences([cleaned_text])


    max_length = 25
    padded_sequence = pad_sequences(sequences, maxlen=max_length, padding='post')


    prediction = model.predict(padded_sequence)[0][0]


    if prediction >= 0.5:
        return "might be depressed"
    else:
        return "most likely not depressed"

"""בודק את המודל שלי עם דוגמאות של עצמי"""

sentence_1 = "I feel very bad. I had a nightmare and I cant take it anymore."
sentence_2 = "i am so  happy, all i want to do is to smile"
print("sentence number 1 is a man who",predict_sentiment(sentence_1))
print("sentence number 2 is a man who",predict_sentiment(sentence_2))

"""ממשיך לבחון את המודל עם דוגמאות שלי"""

user_input = input("How do you feel in these past few days? ")


prediction = predict_sentiment(user_input)


print(f"Prediction: {prediction}")

user_input = input("How do you feel in these past few days? ")


prediction = predict_sentiment(user_input)


print(f"Prediction: {prediction}")

while True:
  user_input = input("How do you feel in these past few days? ")
  if user_input == "exit":
    break
  prediction = predict_sentiment(user_input)
  print(f"Prediction: {prediction}")

while True:
  user_input = input("How do you feel in these past few days? ")
  if user_input == "exit":
    break
  prediction = predict_sentiment(user_input)
  print(f"Prediction: {prediction}")